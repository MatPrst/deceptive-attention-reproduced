{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.1.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ardsnijders/opt/anaconda3/envs/attention/lib/python3.8/site-packages (from transformers) (4.55.1)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Using cached tokenizers-0.9.4-cp38-cp38-macosx_10_11_x86_64.whl (2.0 MB)\n",
      "Processing /Users/ardsnijders/Library/Caches/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677/sacremoses-0.0.43-py3-none-any.whl\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp38-cp38-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/ardsnijders/opt/anaconda3/envs/attention/lib/python3.8/site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: requests in /Users/ardsnijders/opt/anaconda3/envs/attention/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: numpy in /Users/ardsnijders/opt/anaconda3/envs/attention/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: six in /Users/ardsnijders/opt/anaconda3/envs/attention/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /Users/ardsnijders/opt/anaconda3/envs/attention/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/ardsnijders/opt/anaconda3/envs/attention/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ardsnijders/opt/anaconda3/envs/attention/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ardsnijders/opt/anaconda3/envs/attention/lib/python3.8/site-packages (from requests->transformers) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ardsnijders/opt/anaconda3/envs/attention/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Installing collected packages: tokenizers, regex, click, joblib, sacremoses, filelock, transformers\n",
      "Successfully installed click-7.1.2 filelock-3.0.12 joblib-1.0.0 regex-2020.11.13 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertSelfAttention_edit(\n",
      "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "import transformers\n",
    "import math\n",
    "import time\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "class BertSelfAttention_edit(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        if encoder_hidden_states is not None:\n",
    "            mixed_key_layer = self.key(encoder_hidden_states)\n",
    "            mixed_value_layer = self.value(encoder_hidden_states)\n",
    "            attention_mask = encoder_attention_mask\n",
    "        else:\n",
    "            mixed_key_layer = self.key(hidden_states)\n",
    "            mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(attention_probs.shape)\n",
    "        \n",
    "        print(attention_probs[0][0].shape)\n",
    "        sys.exit()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "        return outputs\n",
    "\n",
    "# Store the model we want to use\n",
    "MODEL_NAME = \"bert-base-cased\"\n",
    "\n",
    "# We need to create the model and tokenizer\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "transformers.models.bert.modeling_bert.BertSelfAttention = BertSelfAttention_edit\n",
    "\n",
    "print(model._modules['encoder'].layer[0].attention.self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.shape:  torch.Size([14])\n",
      "torch.Size([1, 12, 14, 14])\n",
      "tensor([[5.8149e-01, 3.1003e-02, 5.6809e-03, 8.5177e-03, 3.7646e-03, 8.1945e-03,\n",
      "         2.1174e-02, 9.6092e-03, 9.3909e-03, 1.4542e-02, 5.8352e-03, 5.9892e-03,\n",
      "         6.1406e-03, 2.8867e-01],\n",
      "        [3.7026e-02, 2.2496e-02, 7.4247e-02, 9.6543e-02, 9.2869e-02, 7.9909e-02,\n",
      "         2.2669e-02, 5.9271e-02, 1.5275e-01, 3.4599e-02, 2.8152e-02, 1.0784e-01,\n",
      "         8.9853e-02, 1.0178e-01],\n",
      "        [1.4594e-01, 4.0346e-02, 1.5959e-01, 3.2362e-02, 4.5379e-02, 7.7049e-02,\n",
      "         3.4556e-02, 1.3711e-01, 3.5139e-02, 5.1518e-02, 2.1252e-02, 5.4418e-02,\n",
      "         1.0750e-01, 5.7844e-02],\n",
      "        [9.3867e-02, 3.2457e-02, 8.2730e-02, 4.9417e-02, 5.2558e-02, 1.5068e-01,\n",
      "         4.1737e-02, 7.8025e-02, 5.6860e-02, 6.0479e-02, 3.6059e-02, 1.0329e-01,\n",
      "         9.7950e-02, 6.3896e-02],\n",
      "        [8.8856e-02, 3.2208e-02, 1.1206e-01, 6.3852e-02, 3.7863e-02, 5.9716e-02,\n",
      "         1.9892e-02, 8.8804e-02, 8.1042e-02, 4.4853e-02, 3.2479e-02, 9.6449e-02,\n",
      "         9.5617e-02, 1.4631e-01],\n",
      "        [3.6784e-01, 6.6611e-02, 3.2849e-02, 4.0393e-02, 2.3714e-02, 1.5655e-02,\n",
      "         4.7767e-02, 6.2145e-02, 4.8346e-02, 4.6849e-02, 3.9193e-02, 5.8524e-02,\n",
      "         4.3519e-02, 1.0659e-01],\n",
      "        [4.8173e-02, 4.1598e-02, 8.2429e-02, 7.4873e-02, 7.6583e-02, 8.7595e-02,\n",
      "         2.7973e-02, 5.0730e-02, 1.1042e-01, 6.0520e-02, 2.8441e-02, 1.2897e-01,\n",
      "         9.2601e-02, 8.9095e-02],\n",
      "        [1.0447e-01, 3.4249e-02, 4.7143e-02, 9.3413e-02, 6.7099e-02, 6.7501e-02,\n",
      "         1.8382e-02, 9.4847e-02, 4.6694e-02, 3.1539e-02, 4.6493e-02, 8.6650e-02,\n",
      "         6.2123e-02, 1.9940e-01],\n",
      "        [3.3377e-02, 4.0866e-02, 1.2930e-01, 7.9788e-02, 6.0342e-02, 8.6663e-02,\n",
      "         2.8469e-02, 7.5949e-02, 6.9640e-02, 5.7514e-02, 2.6459e-02, 9.7264e-02,\n",
      "         1.5174e-01, 6.2630e-02],\n",
      "        [7.0803e-02, 2.2813e-02, 1.1167e-01, 5.6909e-02, 9.4181e-02, 7.1142e-02,\n",
      "         2.9703e-02, 5.2658e-02, 1.0308e-01, 6.6121e-02, 3.4145e-02, 1.4291e-01,\n",
      "         5.1359e-02, 9.2513e-02],\n",
      "        [9.5931e-02, 1.8745e-02, 5.0434e-02, 1.2187e-01, 7.4310e-02, 8.8737e-02,\n",
      "         2.4633e-02, 6.1668e-02, 9.2290e-02, 4.5219e-02, 5.3786e-02, 9.9686e-02,\n",
      "         5.9670e-02, 1.1302e-01],\n",
      "        [7.5388e-02, 2.6516e-02, 8.6020e-02, 2.9929e-02, 7.1407e-02, 1.2653e-01,\n",
      "         3.9498e-02, 8.0887e-02, 5.7719e-02, 6.7293e-02, 1.5772e-02, 5.1026e-02,\n",
      "         1.9307e-01, 7.8943e-02],\n",
      "        [2.3048e-01, 3.2028e-02, 2.2325e-02, 7.9247e-02, 1.4583e-01, 4.3515e-02,\n",
      "         2.2578e-02, 5.2672e-02, 4.8839e-02, 3.1481e-02, 5.5822e-02, 9.1391e-02,\n",
      "         4.4029e-02, 9.9763e-02],\n",
      "        [2.0836e-01, 8.8977e-03, 4.8978e-04, 8.3979e-03, 6.3261e-03, 8.4812e-03,\n",
      "         3.9798e-03, 2.1643e-03, 7.9130e-04, 1.0417e-03, 1.2399e-03, 2.2381e-03,\n",
      "         7.3491e-04, 7.4686e-01]])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#put the model in eval mode\n",
    "model.eval()\n",
    "\n",
    "sent = \"The animal did not cross the street because it was too tired\"\n",
    "#Add CLS and SEP Tokens considering it as one sentence\n",
    "sent = \"[CLS] \" + sent + \" [SEP]\"\n",
    "\n",
    "#apply tokenizer and get ids\n",
    "tokenized_text = tokenizer.tokenize(sent)\n",
    "tokenized_text_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "#Convert to torch tensor\n",
    "input = torch.tensor(tokenized_text_ids)\n",
    "\n",
    "print(\"input.shape: \", input.shape) #torch.Size([15]) , N=15 here\n",
    "input = input.unsqueeze(dim=0) #add a dimension \n",
    "\n",
    "#Fwd pass - get output\n",
    "output = model(input) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to create generic Dataset object, code to create dataloaders from Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class OccupationDataset(Dataset):\n",
    "    \"\"\"Occupation Classification dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, anonymization=False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            \n",
    "        \"\"\"\n",
    "        self.path = './src/classification_tasks/data/occupation-classification/' + dataset + '.txt'     \n",
    "        self.text = pd.read_csv(self.path, sep=\"\\t\", header=None)\n",
    "        self.transform = transform\n",
    "        self.anonymization = anonymization\n",
    "        self.impermissible = [\"he\", \"she\", \"her\", \"his\", \"him\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        label, sentence = self.text.iloc[idx]\n",
    "        \n",
    "        if self.anonymization:\n",
    "            \n",
    "            splitted_sentence = sentence.split()\n",
    "            sentence = ' '.join([i for i in splitted_sentence if i not in self.impermissible])\n",
    "\n",
    "        sample = {'sentence' : sentence, 'label' : label}\n",
    "        return sample\n",
    "\n",
    "occupation_train = OccupationDataset(dataset='train', anonymization=True)\n",
    "occupation_train_loader = torch.utils.data.DataLoader(occupation_train, batch_size=2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['has subspecialty interests in arthroscopic/keyhole and sports surgery, knee and hip replacements, cartilage regeneration and stem cell therapeutics in orthopaedics.', 'ms. staples practices medicine in spring lake park, mn and specializes in orthopaedic surgery. ms. staples is affiliated with unity hospital, north memorial medical center and mercy hospital.']\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(occupation_train_loader):\n",
    "    \n",
    "    labels = batch['label']\n",
    "    sents = batch['sentence']\n",
    "    \n",
    "    print(sents)\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pseudocode of possible train loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def tokenize_batch(sents):\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(sents)\n",
    "    tokenized_text_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    return tokenized_text, tokenized_te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she\n",
      "graduated\n",
      "from\n",
      "the\n",
      "university\n",
      "of\n",
      "ca\n",
      "##li\n",
      "##fo\n",
      "##rn\n",
      "##ia\n",
      ",\n",
      "da\n",
      "##vis\n",
      "in\n",
      "2004\n",
      "and\n",
      "is\n",
      "certified\n",
      "by\n",
      "the\n",
      "national\n",
      "commission\n",
      "on\n",
      "certification\n",
      "of\n",
      "physician\n",
      "assistants\n",
      ".\n",
      "p\n",
      "##ors\n",
      "##cha\n",
      "moved\n",
      "to\n",
      "the\n",
      "ta\n",
      "##hoe\n",
      "/\n",
      "truck\n",
      "##ee\n",
      "area\n",
      "in\n",
      "1996\n",
      "and\n",
      "joined\n",
      "ta\n",
      "##hoe\n",
      "forest\n",
      "health\n",
      "system\n",
      "in\n",
      "2011\n",
      ".\n",
      "m\n",
      "##s\n",
      ".\n",
      "sick\n",
      "##els\n",
      "practices\n",
      "medicine\n",
      "in\n",
      "color\n",
      "##ado\n",
      "springs\n",
      ",\n",
      "co\n",
      "and\n",
      "specializes\n",
      "in\n",
      "internal\n",
      "medicine\n",
      ".\n",
      "m\n",
      "##s\n",
      ".\n",
      "sick\n",
      "##els\n",
      "is\n",
      "affiliated\n",
      "with\n",
      "pen\n",
      "##rose\n",
      "saint\n",
      "f\n",
      "##ran\n",
      "##cis\n",
      "health\n",
      "services\n",
      ".\n",
      "m\n",
      "##s\n",
      ".\n",
      "sick\n",
      "##els\n",
      "speaks\n",
      "en\n",
      "##gli\n",
      "##sh\n",
      "and\n",
      "span\n",
      "##ish\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(occupation_train_loader):\n",
    "        \n",
    "        labels = batch['label']\n",
    "        sents = batch['sentence']\n",
    "        \n",
    "        #TODO: Tokenize sentences, convert to ids to sequences of ids\n",
    "        #TODO: Generate self-attention mask for each sequence\n",
    "        \n",
    "        tokenized_sents = tokenize_batch(sents)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(sentence)\n",
    "        \n",
    "#         print(tokenized_text)\n",
    "        \n",
    "        for ids in tokenized_text_ids:\n",
    "            print(tokenizer.decode(ids))\n",
    "        \n",
    "        break\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
