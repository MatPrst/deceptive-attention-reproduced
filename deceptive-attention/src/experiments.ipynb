{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments: Sequence to Sequence\n",
    "\n",
    "This notebook reproduces our reproducibility project during the Fairness, Accountability,\n",
    "Confidentiality and Transparency (FACT) course at University of Amsterdam. Specifically, we reproduce the results from\n",
    "\"Learning to Deceive with Attention-Based Explanations\".\n",
    "\n",
    "While our main code is contained in the folders `classification` and `sequence-to-sequence`, we enable training and\n",
    "visualization via this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'batch_utils'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-0e08a9150c6e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mprettytable\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mPrettyTable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mseq2seq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevaluate_test\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;31m# try:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/uva_artificial_intelligence/p3/fact_ai/project/FACT/deceptive-attention/src/seq2seq/train.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtqdm\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mbatch_utils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mgen_utils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mlog_utils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'batch_utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from seq2seq.train import train, evaluate_test\n",
    "\n",
    "# try:\n",
    "#     import pytorch_lightning as pl\n",
    "# except ModuleNotFoundError: # In case PyTorch Lightning is not installed by default.\n",
    "#     !pip install pytorch-lightning==1.0.3\n",
    "#     import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attentions = ['dot-product', 'uniform', 'no-attention']\n",
    "\n",
    "# original seeds for which the authors trained their seq2seq models\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "coefficients = [0.0, 1.0, 0.1]\n",
    "tasks = ['copy', 'reverse-copy', 'binary-flip', 'en-de']\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training: Attention Manipulation\n",
    "\n",
    "For this part of the experiments attention is computed as dot-product and impermissible words, as defined in our reproducibility report, are penalized.\n",
    "The lambda coefficient (0.0, 0.1 or 1.0) defines respectively if placing attention on these impermissible words is penalized and if so how much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    for coeff in coefficients:\n",
    "        for task in tasks:\n",
    "            train(task, epochs, coeff, seed, batch_size, attentions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Baselines (without Attention)\n",
    "The authors ran experiments with uniform and no attention (ablation studies) and no penalty on impermissible words (loss coefficient 0.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    for task in tasks:\n",
    "        for attention in attentions[1:]:\n",
    "            train(task, epochs, 0.0, seed, batch_size, attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Attention Manipulation\n",
    "\n",
    "- load pretrained models which we have saved and included in github repo / code\n",
    "- visualize some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_model = ['best', 'latest']\n",
    "\n",
    "for seed in seeds:\n",
    "    for coeff in coefficients:\n",
    "        for task in tasks:\n",
    "            loss, acc, attn_mass = evaluate_test(task, coeff, seed, model=evaluate_model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = [['Dot-Product',2,3],['Uniform',5,6],['None',8,9], ['Manipulated',8,9], ['Manipulated',8,9]]\n",
    "data_frame = pd.DataFrame(data, columns=['Attention', 'Bigram Flip: Acc.', 'Bigram Flip: A.M.'])\n",
    "\n",
    "def generate_ascii_table(df):\n",
    "    x = PrettyTable()\n",
    "    x.field_names = df.columns.tolist()\n",
    "    for row in df.values:\n",
    "        x.add_row(row)\n",
    "    print(x)\n",
    "    return x\n",
    "\n",
    "generate_ascii_table(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Baselines (without Attention)\n",
    "- load pretrained models which we have saved and included in github repo / code\n",
    "- visualize some results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## English-German Translation"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "PyCharm (FACT)",
   "language": "python",
   "name": "pycharm-f84e84a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}