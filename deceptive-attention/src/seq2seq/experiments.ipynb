{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments: Sequence to Sequence\n",
    "\n",
    "This notebook reproduces our reproducibility project during the Fairness, Accountability,\n",
    "Confidentiality and Transparency (FACT) course at University of Amsterdam. Specifically, we reproduce the results from\n",
    "\"Learning to Deceive with Attention-Based Explanations\".\n",
    "\n",
    "While our main code is contained in the folders `classification` and `sequence-to-sequence`, we enable training and\n",
    "visualization via this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--task {copy,reverse-copy,binary-flip,en-hi,en-de}]\n",
      "                             [--debug] [--loss-coef LOSS_COEFF]\n",
      "                             [--epochs EPOCHS] [--seed SEED]\n",
      "                             [--attention ATTENTION] [--batch-size BATCH_SIZE]\n",
      "                             [--num-train NUM_TRAIN] [--decode-with-no-attn]\n",
      "                             [--tensorboard_log]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/rhabacker/Library/Jupyter/runtime/kernel-6c287c0c-3b98-4020-9ba7-a7138db03206.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from train import train\n",
    "# from seq2seq.train import evaluate_test\n",
    "\n",
    "# try:\n",
    "#     import pytorch_lightning as pl\n",
    "# except ModuleNotFoundError: # In case PyTorch Lightning is not installed by default.\n",
    "#     !pip install pytorch-lightning==1.0.3\n",
    "#     import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attentions = ['dot-product', 'uniform', 'no-attention']\n",
    "\n",
    "# original seeds for which the authors trained their seq2seq models\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "coefficients = [0.0, 1.0, 0.1]\n",
    "tasks = ['copy', 'reverse-copy', 'binary-flip', 'en-de']\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training: Attention Manipulation\n",
    "\n",
    "For this part of the experiments' attention is computed as dot-product and impermissible words, as defined in our reproducibility report, are penalized.\n",
    "The lambda coefficient (0.0, 0.1 or 1.0) defines respectively if placing attention on these impermissible words is penalized and if so how much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seeds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-765da6bc32dd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mseed\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mseeds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mcoeff\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcoefficients\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtask\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtasks\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m             \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcoeff\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattentions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'seeds' is not defined"
     ]
    }
   ],
   "source": [
    "for seed in seeds[0:1]:\n",
    "    for coeff in coefficients[0:1]:\n",
    "        for task in tasks[0:1]:\n",
    "            train(task, epochs, coeff, seed, batch_size, attentions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Baselines (without Attention)\n",
    "The authors ran experiments with uniform and no attention (ablation studies) and no penalty on impermissible words (loss coefficient 0.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for seed in seeds[0:1]:\n",
    "    for task in tasks[0:1]:\n",
    "        for attention in attentions[1:]:\n",
    "            train(task, epochs, 0.0, seed, batch_size, attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Attention Manipulation\n",
    "\n",
    "- load pretrained models which we have saved and included in github repo / code\n",
    "- visualize some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate_model = ['best', 'latest']\n",
    "#\n",
    "# for seed in seeds:\n",
    "#     for coeff in coefficients:\n",
    "#         for task in tasks:\n",
    "#             loss, acc, attn_mass = evaluate_test(task, coeff, seed, model=evaluate_model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = [['Dot-Product',2,3],['Uniform',5,6],['None',8,9], ['Manipulated',8,9], ['Manipulated',8,9]]\n",
    "data_frame = pd.DataFrame(data, columns=['Attention', 'Bigram Flip: Acc.', 'Bigram Flip: A.M.'])\n",
    "\n",
    "def generate_ascii_table(df):\n",
    "    x = PrettyTable()\n",
    "    x.field_names = df.columns.tolist()\n",
    "    for row in df.values:\n",
    "        x.add_row(row)\n",
    "    print(x)\n",
    "    return x\n",
    "\n",
    "generate_ascii_table(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Baselines (without Attention)\n",
    "- load pretrained models which we have saved and included in github repo / code\n",
    "- visualize some results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## English-German Translation"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "PyCharm (FACT)",
   "language": "python",
   "name": "pycharm-f84e84a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}