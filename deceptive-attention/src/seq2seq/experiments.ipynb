{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments: Sequence to Sequence\n",
    "\n",
    "This notebook reproduces our reproducibility project during the Fairness, Accountability,\n",
    "Confidentiality and Transparency (FACT) course at University of Amsterdam. Specifically, we reproduce the results from\n",
    "\"Learning to Deceive with Attention-Based Explanations\".\n",
    "\n",
    "While our main code is contained in the folders `classification` and `sequence-to-sequence`, we enable training and\n",
    "visualization via this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from run_experiments_util import run_synthetic_experiments, run_en_de_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attentions = ['dot-product', 'uniform', 'no-attention']\n",
    "\n",
    "# same seeds the authors used. Reported numbers are averaged over seeds for a certain configuration.\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "\n",
    "coefficients = [0.0, 1.0, 0.1]\n",
    "\n",
    "tasks = ['copy', 'reverse-copy', 'binary-flip']\n",
    "\n",
    "# Models are trained UP TO 30 epochs, although we have an early stopping mechanism and the most of them converge until ~15 epochs\n",
    "epochs = 30\n",
    "\n",
    "# per default the models are trained on 1000000 sentences. This can reduce runtime by lowering the amount of sentences trained on\n",
    "num_sentences_train=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training + Evaluation\n",
    "\n",
    "For this part of the experiments' attention is computed as dot-product and impermissible words, as defined in our reproducibility report, are penalized.\n",
    "The lambda coefficient (0.0, 0.1 or 1.0) defines respectively if placing attention on these impermissible words is penalized and if so how much.\n",
    "\n",
    "The authors also ran experiments with uniform and no attention (ablation studies) and no penalty on impermissible words (loss coefficient 0.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sequence Copy, Sequence Reverse and Bigram Flip\n",
    "run_synthetic_experiments(clear_out=True,\n",
    "                          seeds=seeds,\n",
    "                          tasks=tasks,\n",
    "                          coefficients=coefficients,\n",
    "                          attentions=attentions,\n",
    "                          epochs=epochs,\n",
    "                          num_sentences_train=num_sentences_train)\n",
    "\n",
    "# English German Machine Translation\n",
    "run_en_de_experiments(clear_out=True,\n",
    "                      seeds=seeds,\n",
    "                      coefficients=coefficients,\n",
    "                      attentions=attentions,\n",
    "                      epochs=epochs,\n",
    "                      num_sentences_train=num_sentences_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Because our models take quite some time to be trained and this might not be feasable on your machine, the following cells provide\n",
    "functionalities to load and test pretrained models.\n",
    "\n",
    "- add instruction where do get models from\n",
    "\n",
    "### Sequence Copy, Sequence Reverse and Bigram Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: coeff: 0.1 seed: 1 attention: dot-product device: cpu task: copy\n",
      "Could not find file. Proceeding to next model.\n",
      "Configuration: coeff: 0.1 seed: 2 attention: dot-product device: cpu task: copy\n",
      "Could not find file. Proceeding to next model.\n",
      "Configuration: coeff: 0.1 seed: 3 attention: dot-product device: cpu task: copy\n",
      "Could not find file. Proceeding to next model.\n",
      "Configuration: coeff: 0.1 seed: 4 attention: dot-product device: cpu task: copy\n",
      "Could not find file. Proceeding to next model.\n",
      "Configuration: coeff: 0.1 seed: 5 attention: dot-product device: cpu task: copy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-5cb60621870e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m run_synthetic_experiments(clear_out=True,\n\u001B[0m\u001B[1;32m      2\u001B[0m                           \u001B[0mstage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'test'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m                           \u001B[0mseeds\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mseeds\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m                           \u001B[0mtasks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtasks\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                           \u001B[0mcoefficients\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcoefficients\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/uva_artificial_intelligence/p3/fact_ai/project/FACT/deceptive-attention/src/seq2seq/run_experiments_util.py\u001B[0m in \u001B[0;36mrun_synthetic_experiments\u001B[0;34m(clear_out, stage, seeds, tasks, coefficients, attentions, epochs)\u001B[0m\n\u001B[1;32m    221\u001B[0m                 \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"$\\\\lambda$\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcoefficient\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 223\u001B[0;31m                 \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrun_experiment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseeds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcoefficient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    224\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m                 \u001B[0mtask_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtask_names\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtask\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/uva_artificial_intelligence/p3/fact_ai/project/FACT/deceptive-attention/src/seq2seq/run_experiments_util.py\u001B[0m in \u001B[0;36mrun_experiment\u001B[0;34m(task, epochs, seed, coefficient, attention, stage)\u001B[0m\n\u001B[1;32m    110\u001B[0m             \u001B[0macc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0matt_mass\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbleu_score\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcoefficient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_train\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    111\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mstage\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'test'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 112\u001B[0;31m             \u001B[0macc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0matt_mass\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbleu_score\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mevaluate_test\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcoefficient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    113\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    114\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0macc\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/uva_artificial_intelligence/p3/fact_ai/project/FACT/deceptive-attention/src/seq2seq/run_experiments_util.py\u001B[0m in \u001B[0;36mevaluate_test\u001B[0;34m(task, coefficient, seed, attention, batch_size)\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[0msentences\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minitialize_sentences\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdebug\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_train\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100000\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msplits\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mSPLITS\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 59\u001B[0;31m     \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_batches\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_batches_from_sentences\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSRC_LANG\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTRG_LANG\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m     \u001B[0;31m# load the best model for the given settings\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/uva_artificial_intelligence/p3/fact_ai/project/FACT/deceptive-attention/src/seq2seq/batch_utils.py\u001B[0m in \u001B[0;36mget_batches_from_sentences\u001B[0;34m(sentences, batch_size, source_lang, target_lang)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0mdev_batches\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_batches\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdev_sentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msource_lang\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_lang\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m     \u001B[0mtest_batches\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_batches\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_sentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msource_lang\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_lang\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtrain_batches\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdev_batches\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_batches\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/uva_artificial_intelligence/p3/fact_ai/project/FACT/deceptive-attention/src/seq2seq/batch_utils.py\u001B[0m in \u001B[0;36mget_batches\u001B[0;34m(sentences, batch_size, source_lang, target_lang)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     99\u001B[0m         \u001B[0;31m# numpy them\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 100\u001B[0;31m         \u001B[0msrc_sample\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc_sample\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    101\u001B[0m         \u001B[0mtrg_sample\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrg_sample\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    102\u001B[0m         \u001B[0maligned_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maligned_outputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "run_synthetic_experiments(clear_out=True,\n",
    "                          stage='test',\n",
    "                          seeds=seeds,\n",
    "                          tasks=tasks,\n",
    "                          coefficients=coefficients,\n",
    "                          attentions=attentions,\n",
    "                          epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### English-German Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: coeff: 0.0 seed: 1 attention: dot-product device: cpu task: en-de\n",
      "Loading model with path: data/models/model_en-de_attention=dot-product_seed=1_coeff=0.0_epoch=7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:25<00:00,  3.24s/it]\n",
      " 76%|███████▌  | 758/1000 [01:26<00:24,  9.79it/s]"
     ]
    }
   ],
   "source": [
    "run_en_de_experiments(clear_out=True, stage='test', seeds=[1,2])"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "PyCharm (FACT)",
   "language": "python",
   "name": "pycharm-f84e84a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}