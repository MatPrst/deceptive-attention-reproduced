#!/bin/bash

#SBATCH --partition=gpu_titanrtx_shared_course
#SBATCH --gres=gpu:1
#SBATCH --job-name=seq2seq_job
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --time=24:00:00
#SBATCH --mem=32000M
#SBATCH --array=1-20%2
#SBATCH --output=job_outputs/slurm_seq2seq_%A_%a.out

module purge
module load 2019
module load Python
module load Anaconda3

source activate attention

PARAMETERS_FILE=job_parameters/no_attention.txt

srun python -u train.py $(head -$SLURM_ARRAY_TASK_ID $PARAMETERS_FILE | tail -1)
